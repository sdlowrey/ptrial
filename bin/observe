#!/usr/bin/env python
# Start a ptrial observer and convert the data to CSV text on stdout

import argparse
from ptrial.observer.core import PYTHON_DATA, CSV_DATA, JSON_DATA, ASCII_TIME
from ptrial.observer.kernel import StorageObserver, MemoryObserver
from Queue import Queue, Empty
from threading import Thread
import time

SUMMARY_HELP = """Poll system resources at regular intervals and periodically write
the observations to storage."""

DURATION_HELP = 'Time that the program should run (seconds)'
INTERVAL_HELP = 'Observation polling interval (seconds)'
NAME_HELP = 'Name of this trial run; used for something cool'
WRITE_INTERVAL_HELP = 'Data write interval (seconds)'
DESTDIR_HELP = 'Output directory'
ENCODE_HELP = 'Data output encoding'

class ObserverProto(object):
    """
    This is a prototype for exploring an interface/controller for working with many
    Observer threads.
    
    Might move the data formatting functions in core/observer.py to this class.
    
    Args:
      observer : a LoopObserver to be run in a thread
      data_fmt : format of the data returned by get() [default PYTHON_DATA]
      
    Methods:
      start : start the observer's loop
      get : get all the data currently in the Observer's output queue
      
    """
    def __init__(self, observer, data_fmt=PYTHON_DATA):
        self._obs = observer
        self._data_fmt = data_fmt

        # create a Thread and Queue for communicating with the thread
        self._thread = Thread(target=self._obs.run)
        self._q = Queue()
        
        # share the queue with the observer;
        self._obs.queue = self._q
        
    def start(self):
        """Start the thread, which runs the observer."""
        self._thread.start()
        
    def stop(self):
        """Stop the thread, throw away data remaining in the queue."""
        self._obs.stop()
        while not self._q.empty():
            self._q.get()
            self._q.task_done()
            
        # since the workers feeding the main process rather than being fed,
        # the join() may be pointless.
        self._q.join()
        
    def get(self):
        """
        Return all the data in the queue.
        
        This method will run until the queue is empty.
        """
        out_data = []
        while True:
            try:
                out_data.append(self._q.get(block=False))
                self._q.task_done()
            except Empty:
                break
        return out_data
    
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=SUMMARY_HELP)
    parser.add_argument('trial_name', metavar='NAME', help=NAME_HELP)
    parser.add_argument('--duration', default=5, help=DURATION_HELP)
    parser.add_argument('--interval', default=1, help=INTERVAL_HELP)
    parser.add_argument('--destdir', default='/tmp', help=DESTDIR_HELP)
    parser.add_argument('--write-interval', default=5, help=WRITE_INTERVAL_HELP)
    parser.add_argument('--encode', choices=['csv','json','python'], default='csv', help=ENCODE_HELP)
    args = parser.parse_args()
    interval = int(args.interval)  #TODO get parser to cast this
    duration = int(args.duration)
    encode_map = {'csv': CSV_DATA, 'json': JSON_DATA, 'python': PYTHON_DATA}
    encode = encode_map[args.encode]
    
    ##obs = StorageObserver(observer_name, q, mount_point,
    ##                      time_format=ASCII_TIME, data_format=CSV_DATA)
    obs = MemoryObserver('mem_observer', time_format=ASCII_TIME, data_format=encode)
    proto1 = ObserverProto(obs)
    proto1.start()
    for i in range(duration / interval):
        time.sleep(interval)
        data = proto1.get()
        print data
    proto1.stop()
    
    #print obs.field_names
    #for i in range(run_duration / output_interval):
        #time.sleep(output_interval)
        #while True:
            #try:
                #data = q.get(block=False)
                #print data.encode('utf-8')
                #q.task_done()
            #except Empty:
                #break
    #obs.stop()
    #while not q.empty():
        #q.get()
        #q.task_done()
    #q.join()
    # complete
